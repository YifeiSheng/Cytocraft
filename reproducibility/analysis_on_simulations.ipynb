{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pickle, copy\n",
    "import scanpy as sc\n",
    "from scipy.sparse import spmatrix, issparse, csr_matrix\n",
    "from anndata import AnnData\n",
    "from typing import Optional, Union\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import linalg as LA\n",
    "from math import cos, sin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./results/Simulation/HMEC/SimulationResults.csv', header=0)\n",
    "df['RE']=df['RMSD']**2*100/113.9  # RE = RMSD² × n_genes / ||C_benchmark||²\n",
    "df['Resolution'] = (10000/df['Resolution']).astype(int)\n",
    "df['CellNumber/Resolution'] = df['CellNumber'].astype(str) +'/'+ df['Resolution'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.RE.max())\n",
    "print(df.RE.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['CaptureRate'] == 0.5) & (df['DropRate'] == 0.3)].RMSD.median() - df[(df['CaptureRate'] == 0.1) & (df['DropRate'] == 0.6)].RMSD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_25_noise_5 = df[df['CaptureRate'] == 0.1].RE.quantile(0.25)\n",
    "quantile_75_noise_5 = df[df['CaptureRate'] == 0.1].RE.quantile(0.75)\n",
    "median_noise_5 = df[df['CaptureRate'] == 0.1].RE.median()\n",
    "\n",
    "quantile_25_noise_0 = df[df['CaptureRate'] == 0.5].RE.quantile(0.25)\n",
    "quantile_75_noise_0 = df[df['CaptureRate'] == 0.5].RE.quantile(0.75)\n",
    "median_noise_0 = df[df['CaptureRate'] == 0.5].RE.median()\n",
    "\n",
    "diff_median = median_noise_5 - median_noise_0\n",
    "\n",
    "round(quantile_25_noise_5, 4), round(quantile_75_noise_5, 4), round(median_noise_5, 4), round(quantile_25_noise_0, 4), round(quantile_75_noise_0, 4), round(median_noise_0, 4), round(diff_median, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantile_25_noise_5 = df[df['DropRate'] == 0.3].RE.quantile(0.25)\n",
    "#quantile_75_noise_5 = df[df['DropRate'] == 0.3].RE.quantile(0.75)\n",
    "#median_noise_5 = df[df['DropRate'] == 0.3].RE.median()\n",
    "#\n",
    "#quantile_25_noise_0 = df[df['DropRate'] == 0.6].RE.quantile(0.25)\n",
    "#quantile_75_noise_0 = df[df['DropRate'] == 0.6].RE.quantile(0.75)\n",
    "#median_noise_0 = df[df['DropRate'] == 0.6].RE.median()\n",
    "#\n",
    "#diff_median = median_noise_5 - median_noise_0\n",
    "#\n",
    "#round(quantile_25_noise_5, 4), round(quantile_75_noise_5, 4), round(median_noise_5, 4), round(quantile_25_noise_0, 4), round(quantile_75_noise_0, 4), round(median_noise_0, 4), round(diff_median, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_25_noise_5 = df[df['DropRate'] == 0.3].RE.quantile(0.25)\n",
    "quantile_75_noise_5 = df[df['DropRate'] == 0.3].RE.quantile(0.75)\n",
    "median_noise_5 = df[df['DropRate'] == 0.3].RE.median()\n",
    "\n",
    "quantile_25_noise_0 = df[df['DropRate'] == 0.6].RE.quantile(0.25)\n",
    "quantile_75_noise_0 = df[df['DropRate'] == 0.6].RE.quantile(0.75)\n",
    "median_noise_0 = df[df['DropRate'] == 0.6].RE.median()\n",
    "\n",
    "diff_median = median_noise_5 - median_noise_0\n",
    "\n",
    "round(quantile_25_noise_5, 4), round(quantile_75_noise_5, 4), round(median_noise_5, 4), round(quantile_25_noise_0, 4), round(quantile_75_noise_0, 4), round(median_noise_0, 4), round(diff_median, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_25_noise_5 = df[df['Noise'] == 5].RE.quantile(0.25)\n",
    "quantile_75_noise_5 = df[df['Noise'] == 5].RE.quantile(0.75)\n",
    "median_noise_5 = df[df['Noise'] == 5].RE.median()\n",
    "\n",
    "quantile_25_noise_0 = df[df['Noise'] == 0].RE.quantile(0.25)\n",
    "quantile_75_noise_0 = df[df['Noise'] == 0].RE.quantile(0.75)\n",
    "median_noise_0 = df[df['Noise'] == 0].RE.median()\n",
    "\n",
    "diff_median = median_noise_5 - median_noise_0\n",
    "\n",
    "round(quantile_25_noise_5, 4), round(quantile_75_noise_5, 4), round(median_noise_5, 4), round(quantile_25_noise_0, 4), round(quantile_75_noise_0, 4), round(median_noise_0, 4), round(diff_median, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_25_noise_5 = df[df['Resolution'] == 1000].RE.quantile(0.25)\n",
    "quantile_75_noise_5 = df[df['Resolution'] == 1000].RE.quantile(0.75)\n",
    "median_noise_5 = df[df['Resolution'] == 1000].RE.median()\n",
    "\n",
    "quantile_25_noise_0 = df[df['Resolution'] == 333].RE.quantile(0.25)\n",
    "quantile_75_noise_0 = df[df['Resolution'] == 333].RE.quantile(0.75)\n",
    "median_noise_0 = df[df['Resolution'] == 333].RE.median()\n",
    "\n",
    "diff_median = median_noise_5 - median_noise_0\n",
    "\n",
    "round(quantile_25_noise_5, 4), round(quantile_75_noise_5, 4), round(median_noise_5, 4), round(quantile_25_noise_0, 4), round(quantile_75_noise_0, 4), round(median_noise_0, 4), round(diff_median, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_25_noise_5 = df[df['CellNumber'] == 1000].RE.quantile(0.25)\n",
    "quantile_75_noise_5 = df[df['CellNumber'] == 1000].RE.quantile(0.75)\n",
    "median_noise_5 = df[df['CellNumber'] == 1000].RE.median()\n",
    "\n",
    "quantile_25_noise_0 = df[df['CellNumber'] == 100].RE.quantile(0.25)\n",
    "quantile_75_noise_0 = df[df['CellNumber'] == 100].RE.quantile(0.75)\n",
    "median_noise_0 = df[df['CellNumber'] == 100].RE.median()\n",
    "\n",
    "diff_median = median_noise_5 - median_noise_0\n",
    "\n",
    "round(quantile_25_noise_5, 4), round(quantile_75_noise_5, 4), round(median_noise_5, 4), round(quantile_25_noise_0, 4), round(quantile_75_noise_0, 4), round(median_noise_0, 4), round(diff_median, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['AnchorGenePercent'] == 40].RE.median() - df[df['AnchorGenePercent'] == 10].RE.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['AnchorGenePercent'] == 40].RE.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise_highreso = df[(df['Noise'] == 5) & (df['Resolution'] == 333)]\n",
    "median_re_value = round(df_nonoise_highreso.RE.median(), 4)\n",
    "median_spearman_value = round(df_nonoise_highreso.SPEARMAN.median(), 4)\n",
    "median_re_value, median_spearman_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise_highreso = df[(df['Noise'] == 5) & (df['Resolution'] == 333)]\n",
    "median_re_value = round(df_nonoise_highreso.RE.median(), 4)\n",
    "median_spearman_value = round(df_nonoise_highreso.SPEARMAN.median(), 4)\n",
    "median_re_value, median_spearman_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highnoise = df[df['Noise'] != 5]\n",
    "median_value = round(df_highnoise.SPEARMAN.median(), 4)\n",
    "quantile_75 = round(df_highnoise.SPEARMAN.quantile(0.75), 4)\n",
    "quantile_25 = round(df_highnoise.SPEARMAN.quantile(0.25), 4)\n",
    "median_value, quantile_25, quantile_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = round(df_highnoise.RE.median(), 4)\n",
    "quantile_75 = round(df_highnoise.RE.quantile(0.75), 4)\n",
    "quantile_25 = round(df_highnoise.RE.quantile(0.25), 4)\n",
    "median_value, quantile_25, quantile_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = round(df.SPEARMAN.median(), 4)\n",
    "quantile_75 = round(df.SPEARMAN.quantile(0.75), 4)\n",
    "quantile_25 = round(df.SPEARMAN.quantile(0.25), 4)\n",
    "median_value, quantile_25, quantile_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = round(df.RE.median(), 4)\n",
    "quantile_75 = round(df.RE.quantile(0.75), 4)\n",
    "quantile_25 = round(df.RE.quantile(0.25), 4)\n",
    "median_value, quantile_25, quantile_75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig2b. Panel Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmap(cmap, ncolor):\n",
    "    # Import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Define the color map\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "\n",
    "    # Get the RGB values of each color in the color map\n",
    "    hex_values=[]\n",
    "    for i in range(ncolor):\n",
    "        hex_values.append(mcolors.rgb2hex(cmap(int(256/ncolor)*i)))\n",
    "\n",
    "    return hex_values\n",
    "\n",
    "get_cmap('Greens',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# getting unique values of Drop rate and Capture rate\n",
    "# df = df.query('AnchorGenePercent==10')\n",
    "\n",
    "droprates = df['DropRate'].unique()\n",
    "capturerates = df['CaptureRate'].unique()\n",
    "\n",
    "# setting the plot dimension\n",
    "fig, axs = plt.subplots(len(capturerates), len(droprates), figsize=(20,10), dpi=600)\n",
    "\n",
    "# iterating through each Subplot\n",
    "for i, capturerate in enumerate(capturerates):\n",
    "    for j, droprate in enumerate(droprates):\n",
    "        \n",
    "        # subsetting data for each subplot\n",
    "        subset = df[(df['DropRate'] == droprate) & (df['CaptureRate'] == capturerate)]\n",
    "        \n",
    "        # creating a strip plot for the subplot\n",
    "        sns.boxplot(x=\"Noise\", y=\"RE\", hue=\"CellNumber/Resolution\", data=subset, ax=axs[i][j], linewidth=0.5, flierprops={\"marker\":'x', \"markersize\":2.5},\n",
    "                    hue_order=['1000/333','500/333','100/333','1000/500','500/500','100/500','1000/1000','500/1000','100/1000'], \n",
    "                    palette={'1000/333': \"#2070b4\", '500/333': \"#6aaed6\", '100/333': \"#c6dbef\",\n",
    "                            '1000/500': \"#228a44\", '500/500': \"#73c476\", '100/500': \"#c7e9c0\",\n",
    "                            '1000/1000': \"#ca181d\", '500/1000': \"#fb694a\", '100/1000': \"#fcbba1\",})\n",
    "        \n",
    "        # setting title\n",
    "        axs[i][j].set_title(f'Detection Efficiency: {capturerate*100}% | Gene Dropout Rate: {droprate*100}%', fontsize=11)\n",
    "        axs[i][j].set_xticklabels([\"None\",\"Low\",\"High\"])\n",
    "        axs[i][j].set_xlabel(\"Noise level\", fontsize=12)\n",
    "        axs[i][j].set_ylabel(\"Relative error\", fontsize=12)\n",
    "        \n",
    "\n",
    "        # removing the legend\n",
    "        axs[i][j].legend([],[], frameon=False)\n",
    "        # set y-axis to log10 scale\n",
    "        axs[i][j].set_yscale('log', base=10, subs=[0,1,2,3])\n",
    "        axs[i][j].set_ylim(0.00001,3.2)\n",
    "        if i==0 and j==3:\n",
    "            axs[i][j].legend(loc='lower center', title=\"Cell Number / Resolution (nm)\", ncol=3)   \n",
    "\n",
    "# setting the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/Fig2c.pdf', bbox_inches='tight', dpi=400)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# getting unique values of Drop rate and Capture rate\n",
    "df_sub = df.query('AnchorGenePercent==10')\n",
    "\n",
    "droprates = df_sub['DropRate'].unique()\n",
    "capturerates = df_sub['CaptureRate'].unique()\n",
    "\n",
    "# setting the plot dimension\n",
    "fig, axs = plt.subplots(len(capturerates), len(droprates), figsize=(20,10), dpi=600)\n",
    "\n",
    "# iterating through each Subplot\n",
    "for i, capturerate in enumerate(capturerates):\n",
    "    for j, droprate in enumerate(droprates):\n",
    "        \n",
    "        # subsetting data for each subplot\n",
    "        subset = df_sub[(df_sub['DropRate'] == droprate) & (df_sub['CaptureRate'] == capturerate)]\n",
    "        \n",
    "        # creating a strip plot for the subplot\n",
    "        sns.boxplot(x=\"Noise\", y=\"RE\", hue=\"CellNumber/Resolution\", data=subset, ax=axs[i][j], linewidth=0.5, flierprops={\"marker\":'x', \"markersize\":1},\n",
    "                    hue_order=['1000/333','500/333','100/333','1000/500','500/500','100/500','1000/1000','500/1000','100/1000'], \n",
    "                    palette={'1000/333': \"#2070b4\", '500/333': \"#6aaed6\", '100/333': \"#c6dbef\",\n",
    "                            '1000/500': \"#228a44\", '500/500': \"#73c476\", '100/500': \"#c7e9c0\",\n",
    "                            '1000/1000': \"#ca181d\", '500/1000': \"#fb694a\", '100/1000': \"#fcbba1\",})\n",
    "        \n",
    "        # setting title\n",
    "        axs[i][j].set_title(f'Detection Efficiency: {capturerate*100}% | Gene Dropout Rate: {droprate*100}%', fontsize=11)\n",
    "        axs[i][j].set_xticklabels([\"None\",\"Low\",\"High\"])\n",
    "        axs[i][j].set_xlabel(\"Noise Level\")\n",
    "        axs[i][j].set_ylabel(\"Relative error\")\n",
    "        axs[i][j].set_ylim(0,0.65)\n",
    "        axs[i][j].legend(loc='upper left', title=\"Cell Number / Resolution (nm)\", ncol=3, fontsize=9.2)\n",
    "        \n",
    "# setting the layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plot for Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by 'group1' and 'group2', calculate sum of 'mirror', and count of rows (not including NaNs in 'mirror')\n",
    "grouped = df.groupby(['Noise','CaptureRate','DropRate','Resolution','CellNumber']).agg(\n",
    "    mirror_sum=('MIRROR', 'sum'),\n",
    "    count=('MIRROR', 'count')\n",
    ")\n",
    "\n",
    "# calculate 'mirror percent'\n",
    "grouped['mirror_percent'] = grouped['mirror_sum'] / grouped['count']\n",
    "print(pd.DataFrame([grouped['mirror_sum'],grouped['count'],grouped['mirror_percent']]).T.mirror_percent.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "args=['CellNumber', 'CaptureRate', 'DropRate', 'Resolution', 'AnchorGenePercent', 'Noise']\n",
    "xlabels=['Cell Number', 'Detection Efficiency', 'Gene Dropout Rate', 'Resolution', 'Anchor Gene Rate', 'Noise Level']\n",
    "# setting the plot dimension\n",
    "fig, axs = plt.subplots(1,len(args), figsize=(12,2), dpi=300)\n",
    "\n",
    "# iterating through each Subplot\n",
    "for i, arg in enumerate(args):\n",
    "        # group by arg, calculate sum of 'mirror', and count of rows (not including NaNs in 'mirror')\n",
    "        grouped = df.groupby([arg]).agg(\n",
    "                mirror_sum=('MIRROR', 'sum'),\n",
    "                count=('MIRROR', 'count')\n",
    "        )\n",
    "\n",
    "        # calculate 'mirror percent'\n",
    "        grouped['mirror_percent'] = grouped['mirror_sum'] / grouped['count']\n",
    "        df_mirror = pd.DataFrame([grouped['mirror_sum'],grouped['count'],grouped['mirror_percent']])\n",
    "        \n",
    "        # creating a strip plot for the subplot\n",
    "        sns.barplot(x=df_mirror.T.index.values, y=df_mirror.T.mirror_percent.values, ax=axs[i], palette='Blues')\n",
    "        \n",
    "        # setting title\n",
    "        #axs[i].set_title(f'Capture Rate: {capturerate*100}% | Drop Rate: {droprate*100}%')\n",
    "        #axs[i].set_xticklabels([\"None\",\"Low\",\"High\"])\n",
    "        axs[i].set_xlabel(xlabels[i])\n",
    "        axs[i].set_ylabel(\"Mirror Rate\")\n",
    "        axs[i].set_ylim(0,1)\n",
    "        #axs[i].legend(loc='upper left', title=\"Cell Number / Resolution\", ncol=3)\n",
    "        \n",
    "# setting the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/Simulation_MirrorRate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig2c. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./results/Simulation/HMEC/SimulationResults.csv', header=0)\n",
    "df['RE']=df['RMSD']**2*100/113.9  # RE = RMSD² × n_genes / ||C_benchmark||²\n",
    "df['Resolution'] = (10000/df['Resolution']).astype(int)\n",
    "df['CellNumber/Resolution'] = df['CellNumber'].astype(str) +'/'+ df['Resolution'].astype(str)\n",
    "df['Method'] = 'cytocraft'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.read_csv('./results/Simulation/HMEC/compare_baseline/SimulationCompareResults.csv', header=None)\n",
    "cols = df.columns.tolist()\n",
    "cols[-3] = 'Method'\n",
    "# assign matching number of column names from df to df_compare\n",
    "df_compare.columns = cols[: df_compare.shape[1]] if len(cols) != df_compare.shape[1] else cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare['RE']=df_compare['RMSD']**2*100/113.9  # RE = RMSD² × n_genes / ||C_benchmark||²\n",
    "df_compare['Resolution'] = (10000/df_compare['Resolution']).astype(int)\n",
    "df_compare['CellNumber/Resolution'] = df_compare['CellNumber'].astype(str) +'/'+ df_compare['Resolution'].astype(str)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df and df_compare by concatenation (keep all columns)\n",
    "df_merged = pd.concat([df, df_compare], ignore_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# combined plot: x = evaluation metric, hue = Method\n",
    "metrics = ['RE', 'SPEARMAN']\n",
    "\n",
    "# melt dataframe into long form\n",
    "melted = df_merged[['Method'] + metrics].melt(id_vars='Method',\n",
    "                                             value_vars=metrics,\n",
    "                                             var_name='Evaluation',\n",
    "                                             value_name='Value')\n",
    "# drop missing and remove non-positive RE for clarity\n",
    "melted = melted.dropna(subset=['Value'])\n",
    "melted = melted[~((melted['Evaluation'] == 'RE') & (melted['Value'] <= 0))]\n",
    "\n",
    "# define method order (optional: use a specific order like order_re/order_spear)\n",
    "method_order = list(df_merged['Method'].unique())\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "palette = {'cytocraft': '#E64B34', 'average': '#4EB9D4', 'isomap_k10': '#37A089'}\n",
    "sns.boxplot(x='Evaluation', y='Value', hue='Method', data=melted,\n",
    "            hue_order=method_order, showfliers=False, palette=palette,\n",
    "            width=0.6, linewidth=0.8)\n",
    "\n",
    "# remove duplicate legend (stripplot adds a second legend)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# keep only the first block of handles (one per method)\n",
    "n_methods = len(method_order)\n",
    "plt.legend(handles[:n_methods], labels[:n_methods], title='Method', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# set x tick labels and y limits\n",
    "plt.gca().set_xticklabels(['Relative error', 'Spearman'])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/evaluations_by_method.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw one combined RE / SPEARMAN plot for each noise level\n",
    "noise_levels = sorted(df_merged['Noise'].unique())\n",
    "n = len(noise_levels)\n",
    "fig_width = max(3.1 * n, 4)\n",
    "fig, axs = plt.subplots(1, n, figsize=(fig_width, 3), dpi=300, sharey=True)\n",
    "\n",
    "if n == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for ax, noise in zip(axs, noise_levels):\n",
    "    subset = df_merged[df_merged['Noise'] == noise]\n",
    "    melted_noise = subset[['Method'] + metrics].melt(id_vars='Method',\n",
    "                                                     value_vars=metrics,\n",
    "                                                     var_name='Evaluation',\n",
    "                                                     value_name='Value')\n",
    "    melted_noise = melted_noise.dropna(subset=['Value'])\n",
    "    melted_noise = melted_noise[~((melted_noise['Evaluation'] == 'RE') & (melted_noise['Value'] <= 0))]\n",
    "\n",
    "    sns.boxplot(x='Evaluation', y='Value', hue='Method', data=melted_noise,\n",
    "                hue_order=method_order, showfliers=False, palette=palette,\n",
    "                width=0.6, linewidth=0.8, ax=ax)\n",
    "\n",
    "    ax.set_title(f'Noise = {noise}')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')  # only show ylabel on the first subplot\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xticklabels(['Relative error', 'Spearman'])\n",
    "\n",
    "    # only show legend on the first subplot (keep it outside)\n",
    "    if ax is axs[0]:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles[:len(method_order)], labels[:len(method_order)],\n",
    "                  title='Method', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    else:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/evaluations_by_method_by_noise.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a 2x2 subplot grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(5.2, 6), dpi=300)\n",
    "axs = axs.flatten()  # Flatten the 2D array of axes for easier iteration\n",
    "\n",
    "# --- Plot 1: Overall Comparison (Top-Left Panel) ---\n",
    "metrics = ['RE', 'SPEARMAN']\n",
    "melted = df_merged[['Method'] + metrics].melt(id_vars='Method',\n",
    "                                             value_vars=metrics,\n",
    "                                             var_name='Evaluation',\n",
    "                                             value_name='Value')\n",
    "melted = melted.dropna(subset=['Value'])\n",
    "melted = melted[~((melted['Evaluation'] == 'RE') & (melted['Value'] <= 0))]\n",
    "\n",
    "method_order = list(df_merged['Method'].unique())\n",
    "palette = {'cytocraft': '#E64B34', 'average': '#4EB9D4', 'isomap_k10': '#37A089'}\n",
    "\n",
    "sns.boxplot(x='Evaluation', y='Value', hue='Method', data=melted,\n",
    "            hue_order=method_order, showfliers=False, palette=palette,\n",
    "            width=0.6, linewidth=0.8, ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Overall Comparison')\n",
    "axs[0].set_xticklabels(['Relative error', 'Spearman'])\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].set_ylabel('')\n",
    "axs[0].set_ylim(-0.1, 1.1)\n",
    "axs[0].get_legend().remove() # Remove individual legend\n",
    "\n",
    "# --- Plots 2, 3, 4: Comparison by Noise Level ---\n",
    "noise_levels = sorted(df_merged['Noise'].unique())\n",
    "\n",
    "# Use the next 3 axes for the noise-level plots\n",
    "for i, noise in enumerate(noise_levels[:3]):\n",
    "    ax = axs[i + 1]\n",
    "    subset = df_merged[df_merged['Noise'] == noise]\n",
    "    melted_noise = subset[['Method'] + metrics].melt(id_vars='Method',\n",
    "                                                     value_vars=metrics,\n",
    "                                                     var_name='Evaluation',\n",
    "                                                     value_name='Value')\n",
    "    melted_noise = melted_noise.dropna(subset=['Value'])\n",
    "    melted_noise = melted_noise[~((melted_noise['Evaluation'] == 'RE') & (melted_noise['Value'] <= 0))]\n",
    "\n",
    "    sns.boxplot(x='Evaluation', y='Value', hue='Method', data=melted_noise,\n",
    "                hue_order=method_order, showfliers=False, palette=palette,\n",
    "                width=0.6, linewidth=0.8, ax=ax)\n",
    "\n",
    "    ax.set_title(f'Noise = {noise}')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xticklabels(['Relative error', 'Spearman'])\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# --- Final Figure Adjustments ---\n",
    "# Create a single legend for the entire figure\n",
    "# handles, labels = axs[0].get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, title='Method', bbox_to_anchor=(0.9, 0.5), loc='center left')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make space for the legend\n",
    "plt.savefig('figures/Fig2c.combined_evaluations.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Verify statistical statement about performance differences among methods across noise levels\n",
    "\n",
    "# Ensure required dataframe exists\n",
    "assert 'df_merged' in globals(), \"df_merged not found. Run previous comparison cells first.\"\n",
    "\n",
    "# Identify method names\n",
    "methods = df_merged['Method'].unique().tolist()\n",
    "assert 'cytocraft' in methods, \"cytocraft method missing.\"\n",
    "avg_method = 'average' if 'average' in methods else None\n",
    "iso_methods = [m for m in methods if 'isomap' in m.lower()]\n",
    "iso_method = iso_methods[0] if iso_methods else None\n",
    "assert avg_method is not None, \"Average baseline missing.\"\n",
    "assert iso_method is not None, \"Isomap baseline missing.\"\n",
    "\n",
    "# Count simulations\n",
    "total_runs = len(df_merged)\n",
    "runs_per_noise = df_merged.groupby('Noise').size().to_dict()\n",
    "\n",
    "# Median stats by noise and method\n",
    "median_table = (\n",
    "    df_merged.groupby(['Noise', 'Method'])[['RE', 'SPEARMAN']]\n",
    "    .median()\n",
    "    .unstack('Method')\n",
    ")\n",
    "\n",
    "# Helper to fetch values\n",
    "def get_val(metric, method, noise):\n",
    "    return median_table.loc[noise, (metric, method)]\n",
    "\n",
    "noise_levels = median_table.index.tolist()\n",
    "\n",
    "# Compute fold reductions (RE) and absolute improvements (SPEARMAN)\n",
    "rows = []\n",
    "for n in noise_levels:\n",
    "    re_c = get_val('RE', 'cytocraft', n)\n",
    "    re_a = get_val('RE', avg_method, n)\n",
    "    re_i = get_val('RE', iso_method, n)\n",
    "    sp_c = get_val('SPEARMAN', 'cytocraft', n)\n",
    "    sp_a = get_val('SPEARMAN', avg_method, n)\n",
    "    sp_i = get_val('SPEARMAN', iso_method, n)\n",
    "    rows.append({\n",
    "        'Noise': n,\n",
    "        'Median_RE_cytocraft': re_c,\n",
    "        f'Median_RE_{avg_method}': re_a,\n",
    "        f'Median_RE_{iso_method}': re_i,\n",
    "        'Fold_RE_vs_Average': re_a / re_c if re_c > 0 else float('inf'),\n",
    "        'Fold_RE_vs_Isomap': re_i / re_c if re_c > 0 else float('inf'),\n",
    "        'Delta_Sp_vs_Average': sp_c - sp_a,\n",
    "        'Delta_Sp_vs_Isomap': sp_c - sp_i\n",
    "    })\n",
    "\n",
    "verification_df = pd.DataFrame(rows)\n",
    "\n",
    "# Ranges\n",
    "fold_avg_min, fold_avg_max = verification_df['Fold_RE_vs_Average'].min(), verification_df['Fold_RE_vs_Average'].max()\n",
    "fold_iso_min, fold_iso_max = verification_df['Fold_RE_vs_Isomap'].min(), verification_df['Fold_RE_vs_Isomap'].max()\n",
    "delta_sp_avg_min, delta_sp_avg_max = verification_df['Delta_Sp_vs_Average'].min(), verification_df['Delta_Sp_vs_Average'].max()\n",
    "delta_sp_iso_min, delta_sp_iso_max = verification_df['Delta_Sp_vs_Isomap'].min(), verification_df['Delta_Sp_vs_Isomap'].max()\n",
    "\n",
    "# Claimed ranges\n",
    "claim_fold_avg = (4.2, 11.7)\n",
    "claim_fold_iso = (2.9, 7.5)\n",
    "claim_sp_avg = (0.12, 0.31)\n",
    "claim_sp_iso = (0.08, 0.24)\n",
    "\n",
    "def within(observed_min, observed_max, claimed_min, claimed_max):\n",
    "    return observed_min >= claimed_min - 1e-9 and observed_max <= claimed_max + 1e-9\n",
    "\n",
    "result = {\n",
    "    'Total simulations (rows)': total_runs,\n",
    "    'Runs per noise': runs_per_noise,\n",
    "    'Observed fold RE vs Average (min, max)': (round(fold_avg_min,3), round(fold_avg_max,3)),\n",
    "    'Observed fold RE vs Isomap (min, max)': (round(fold_iso_min,3), round(fold_iso_max,3)),\n",
    "    'Observed ΔSpearman vs Average (min, max)': (round(delta_sp_avg_min,3), round(delta_sp_avg_max,3)),\n",
    "    'Observed ΔSpearman vs Isomap (min, max)': (round(delta_sp_iso_min,3), round(delta_sp_iso_max,3)),\n",
    "    'Claim match RE fold vs Average': within(fold_avg_min, fold_avg_max, *claim_fold_avg),\n",
    "    'Claim match RE fold vs Isomap': within(fold_iso_min, fold_iso_max, *claim_fold_iso),\n",
    "    'Claim match ΔSpearman vs Average': within(delta_sp_avg_min, delta_sp_avg_max, *claim_sp_avg),\n",
    "    'Claim match ΔSpearman vs Isomap': within(delta_sp_iso_min, delta_sp_iso_max, *claim_sp_iso),\n",
    "}\n",
    "\n",
    "print(\"Per-noise medians and derived stats:\")\n",
    "print(verification_df.round(4).to_string(index=False))\n",
    "print(\"\\nSummary verification:\")\n",
    "for k,v in result.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# If any mismatch, flag\n",
    "if not all([result['Claim match RE fold vs Average'],\n",
    "            result['Claim match RE fold vs Isomap'],\n",
    "            result['Claim match ΔSpearman vs Average'],\n",
    "            result['Claim match ΔSpearman vs Isomap']]):\n",
    "    print(\"\\nConclusion: Provided statement does NOT fully match observed ranges.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Provided statement is consistent with observed statistics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Verify statistical statement about performance differences among methods across noise levels\n",
    "\n",
    "# Ensure required dataframe exists\n",
    "assert 'df_merged' in globals(), \"df_merged not found. Run previous comparison cells first.\"\n",
    "\n",
    "# Identify method names\n",
    "methods = df_merged['Method'].unique().tolist()\n",
    "assert 'cytocraft' in methods, \"cytocraft method missing.\"\n",
    "avg_method = 'average' if 'average' in methods else None\n",
    "iso_methods = [m for m in methods if 'isomap' in m.lower()]\n",
    "iso_method = iso_methods[0] if iso_methods else None\n",
    "assert avg_method is not None, \"Average baseline missing.\"\n",
    "assert iso_method is not None, \"Isomap baseline missing.\"\n",
    "\n",
    "# Count simulations\n",
    "total_runs = len(df_merged)\n",
    "runs_per_noise = df_merged.groupby('Noise').size().to_dict()\n",
    "\n",
    "# Median stats by noise and method\n",
    "median_table = (\n",
    "    df_merged.groupby(['Noise', 'Method'])[['RE', 'SPEARMAN']]\n",
    "    .median()\n",
    "    .unstack('Method')\n",
    ")\n",
    "\n",
    "# Helper to fetch values\n",
    "def get_val(metric, method, noise):\n",
    "    return median_table.loc[noise, (metric, method)]\n",
    "\n",
    "noise_levels = median_table.index.tolist()\n",
    "\n",
    "# Compute fold reductions (RE) and absolute improvements (SPEARMAN)\n",
    "rows = []\n",
    "for n in noise_levels:\n",
    "    re_c = get_val('RE', 'cytocraft', n)\n",
    "    re_a = get_val('RE', avg_method, n)\n",
    "    re_i = get_val('RE', iso_method, n)\n",
    "    sp_c = get_val('SPEARMAN', 'cytocraft', n)\n",
    "    sp_a = get_val('SPEARMAN', avg_method, n)\n",
    "    sp_i = get_val('SPEARMAN', iso_method, n)\n",
    "    rows.append({\n",
    "        'Noise': n,\n",
    "        'Median_RE_cytocraft': re_c,\n",
    "        f'Median_RE_{avg_method}': re_a,\n",
    "        f'Median_RE_{iso_method}': re_i,\n",
    "        'Fold_RE_vs_Average': re_a / re_c if re_c > 0 else float('inf'),\n",
    "        'Fold_RE_vs_Isomap': re_i / re_c if re_c > 0 else float('inf'),\n",
    "        'Delta_Sp_vs_Average': sp_c - sp_a,\n",
    "        'Delta_Sp_vs_Isomap': sp_c - sp_i\n",
    "    })\n",
    "\n",
    "verification_df = pd.DataFrame(rows)\n",
    "\n",
    "# Ranges\n",
    "fold_avg_min, fold_avg_max = verification_df['Fold_RE_vs_Average'].min(), verification_df['Fold_RE_vs_Average'].max()\n",
    "fold_iso_min, fold_iso_max = verification_df['Fold_RE_vs_Isomap'].min(), verification_df['Fold_RE_vs_Isomap'].max()\n",
    "delta_sp_avg_min, delta_sp_avg_max = verification_df['Delta_Sp_vs_Average'].min(), verification_df['Delta_Sp_vs_Average'].max()\n",
    "delta_sp_iso_min, delta_sp_iso_max = verification_df['Delta_Sp_vs_Isomap'].min(), verification_df['Delta_Sp_vs_Isomap'].max()\n",
    "\n",
    "# Claimed ranges\n",
    "claim_fold_avg = (4.2, 11.7)\n",
    "claim_fold_iso = (2.9, 7.5)\n",
    "claim_sp_avg = (0.12, 0.31)\n",
    "claim_sp_iso = (0.08, 0.24)\n",
    "\n",
    "def within(observed_min, observed_max, claimed_min, claimed_max):\n",
    "    return observed_min >= claimed_min - 1e-9 and observed_max <= claimed_max + 1e-9\n",
    "\n",
    "result = {\n",
    "    'Total simulations (rows)': total_runs,\n",
    "    'Runs per noise': runs_per_noise,\n",
    "    'Observed fold RE vs Average (min, max)': (round(fold_avg_min,3), round(fold_avg_max,3)),\n",
    "    'Observed fold RE vs Isomap (min, max)': (round(fold_iso_min,3), round(fold_iso_max,3)),\n",
    "    'Observed ΔSpearman vs Average (min, max)': (round(delta_sp_avg_min,3), round(delta_sp_avg_max,3)),\n",
    "    'Observed ΔSpearman vs Isomap (min, max)': (round(delta_sp_iso_min,3), round(delta_sp_iso_max,3)),\n",
    "    'Claim match RE fold vs Average': within(fold_avg_min, fold_avg_max, *claim_fold_avg),\n",
    "    'Claim match RE fold vs Isomap': within(fold_iso_min, fold_iso_max, *claim_fold_iso),\n",
    "    'Claim match ΔSpearman vs Average': within(delta_sp_avg_min, delta_sp_avg_max, *claim_sp_avg),\n",
    "    'Claim match ΔSpearman vs Isomap': within(delta_sp_iso_min, delta_sp_iso_max, *claim_sp_iso),\n",
    "}\n",
    "\n",
    "print(\"Per-noise medians and derived stats:\")\n",
    "print(verification_df.round(4).to_string(index=False))\n",
    "print(\"\\nSummary verification:\")\n",
    "for k,v in result.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# If any mismatch, flag\n",
    "if not all([result['Claim match RE fold vs Average'],\n",
    "            result['Claim match RE fold vs Isomap'],\n",
    "            result['Claim match ΔSpearman vs Average'],\n",
    "            result['Claim match ΔSpearman vs Isomap']]):\n",
    "    print(\"\\nConclusion: Provided statement does NOT fully match observed ranges.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Provided statement is consistent with observed statistics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised, data-driven performance description based on observed statistics\n",
    "noise_label = {0: \"none\", 2: \"low\", 5: \"high\"}\n",
    "\n",
    "# Extract per-noise rows\n",
    "rows_map = {r.Noise: r for _, r in verification_df.iterrows()}\n",
    "\n",
    "n_total = total_runs\n",
    "re_avg_range = (round(fold_avg_min, 3), round(fold_avg_max, 3))\n",
    "re_iso_range = (round(fold_iso_min, 3), round(fold_iso_max, 3))\n",
    "sp_avg_range = (round(delta_sp_avg_min, 3), round(delta_sp_avg_max, 3))\n",
    "sp_iso_range = (round(delta_sp_iso_min, 3), round(delta_sp_iso_max, 3))\n",
    "\n",
    "def fmt_row(noise):\n",
    "    r = rows_map[noise]\n",
    "    return (f\"Noise {noise_label[noise]}: RE fold vs Average={r.Fold_RE_vs_Average:.3f}, \"\n",
    "            f\"RE fold vs Isomap={r.Fold_RE_vs_Isomap:.3f}, \"\n",
    "            f\"ΔSpearman vs Average={r.Delta_Sp_vs_Average:+.3f}, \"\n",
    "            f\"ΔSpearman vs Isomap={r.Delta_Sp_vs_Isomap:+.3f}\")\n",
    "\n",
    "per_noise_lines = \"\\n\".join(fmt_row(n) for n in noise_levels)\n",
    "\n",
    "revised_description = f\"\"\"\n",
    "Across {n_total} simulations (noise levels: none, low, high):\n",
    "• Median relative error (RE) fold change vs Average baseline spans {re_avg_range[0]}–{re_avg_range[1]}:\n",
    "  – Strong improvements at none ({rows_map[0].Fold_RE_vs_Average:.2f}x) and low ({rows_map[2].Fold_RE_vs_Average:.2f}x) noise\n",
    "  – Worse than Average at high noise (fold {rows_map[5].Fold_RE_vs_Average:.2f} < 1)\n",
    "• Median RE fold improvement vs Isomap spans {re_iso_range[0]}–{re_iso_range[1]} (improvement at all noise levels)\n",
    "• Median Spearman change vs Average ranges {sp_avg_range[0]} to {sp_avg_range[1]} (gains at none/low noise, loss at high noise)\n",
    "• Median Spearman gain vs Isomap is consistently positive ({sp_iso_range[0]}–{sp_iso_range[1]})\n",
    "Per-noise detail:\n",
    "{per_noise_lines}\n",
    "\"\"\"\n",
    "\n",
    "print(revised_description.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
